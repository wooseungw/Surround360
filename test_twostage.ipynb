{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea7bf131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seungwoo/anaconda3/envs/sur/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "STAGE 1: Vision-Language Representation Pre-training\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VICReg loss calculation failed: shape '[4, 3, 16, 16, 1408]' is invalid for input of size 4342272\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Blip2QFormerModel.forward() got an unexpected keyword argument 'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m dummy_labels_itm = torch.tensor([\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m] * \u001b[32m4\u001b[39m).to(device) \u001b[38;5;66;03m# [match, no-match] 반복\u001b[39;00m\n\u001b[32m     32\u001b[39m model_stage1.train()\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m output_s1 = \u001b[43mmodel_stage1\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdummy_pixel_values_s1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdummy_input_ids_s1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdummy_attention_mask_s1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels_itm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdummy_labels_itm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverlap_consistency_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\n\u001b[32m     39\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m loss_s1 = output_s1.loss\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# loss_s1.backward() # 실제 학습 시 주석 해제\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# optimizer_s1.step()\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/sur/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/sur/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1536\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1537\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1539\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1540\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1544\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Surround360/src/models/surroundblip.py:113\u001b[39m, in \u001b[36mSurroundBlipForPretraining.forward\u001b[39m\u001b[34m(self, pixel_values, input_ids, attention_mask, labels_itm, overlap_consistency_weight, return_dict)\u001b[39m\n\u001b[32m    110\u001b[39m image_embeds_expanded = image_embeds.repeat_interleave(num_text_samples // B, dim=\u001b[32m0\u001b[39m)\n\u001b[32m    111\u001b[39m image_attention_mask_expanded = torch.ones(image_embeds_expanded.size()[:-\u001b[32m1\u001b[39m], dtype=torch.long, device=image_embeds.device)\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m qformer_outputs_itm = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mqformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_embeds_expanded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_attention_mask_expanded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m itm_features = qformer_outputs_itm.last_hidden_state[:, \u001b[32m0\u001b[39m, :]\n\u001b[32m    122\u001b[39m itm_logits = \u001b[38;5;28mself\u001b[39m.itm_head(itm_features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/sur/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/sur/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1536\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1537\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1539\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1540\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1544\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: Blip2QFormerModel.forward() got an unexpected keyword argument 'input_ids'"
     ]
    }
   ],
   "source": [
    "from transformers import Blip2Config, Blip2QFormerConfig, Blip2VisionConfig\n",
    "from src.models.surroundblip import (\n",
    "    SurroundBlipForPretraining, SurroundBlipForConditionalGeneration\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import logging\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import os\n",
    "    import tempfile\n",
    "    # 이 스크립트는 개념 증명을 위한 것이며, 실제 데이터로더가 필요합니다.\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # --- 1단계: 표현 학습 ---\n",
    "    print(\"=\"*50)\n",
    "    print(\"STAGE 1: Vision-Language Representation Pre-training\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # 1. 모델 및 설정 초기화\n",
    "    stage1_config = Blip2Config()\n",
    "    model_stage1 = SurroundBlipForPretraining(stage1_config).to(device)\n",
    "    optimizer_s1 = torch.optim.AdamW(model_stage1.parameters(), lr=1e-5)\n",
    "\n",
    "    # 2. 가상 데이터로 1단계 학습 시뮬레이션\n",
    "    # 실제로는 데이터로더에서 배치 데이터를 가져와야 합니다.\n",
    "    dummy_pixel_values_s1 = torch.randn(4, 3, 3, 224, 224).to(device) # (B, P, C, H, W)\n",
    "    dummy_input_ids_s1 = torch.randint(100, 30000, (8, 32)).to(device) # 4개 이미지, 각각 긍정/부정 텍스트 -> 8개\n",
    "    dummy_attention_mask_s1 = torch.ones_like(dummy_input_ids_s1)\n",
    "    dummy_labels_itm = torch.tensor([0, 1] * 4).to(device) # [match, no-match] 반복\n",
    "\n",
    "    model_stage1.train()\n",
    "    output_s1 = model_stage1(\n",
    "        pixel_values=dummy_pixel_values_s1,\n",
    "        input_ids=dummy_input_ids_s1,\n",
    "        attention_mask=dummy_attention_mask_s1,\n",
    "        labels_itm=dummy_labels_itm,\n",
    "        overlap_consistency_weight=0.1\n",
    "    )\n",
    "    loss_s1 = output_s1.loss\n",
    "    # loss_s1.backward() # 실제 학습 시 주석 해제\n",
    "    # optimizer_s1.step()\n",
    "    print(f\"Stage 1 Loss: {loss_s1.item():.4f}\")\n",
    "\n",
    "    # 3. 학습된 1단계 모델 저장 (임시 디렉터리에)\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        stage1_save_path = os.path.join(tmpdir, \"surroundblip_stage1_pretrained\")\n",
    "        model_stage1.save_pretrained(stage1_save_path)\n",
    "        print(f\"Stage 1 model saved to: {stage1_save_path}\")\n",
    "\n",
    "        # --- 2단계: 조건부 생성 학습 ---\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"STAGE 2: Conditional Generation Fine-tuning\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # 4. 1단계 가중치를 불러와 2단계 모델 초기화\n",
    "        # strict=False는 1단계 모델에 없는 LLM 가중치는 무시하고 로드하라는 의미\n",
    "        stage2_config = Blip2Config()\n",
    "        model_stage2 = SurroundBlipForConditionalGeneration.from_pretrained(\n",
    "            stage1_save_path, config=stage2_config, strict=False\n",
    "        ).to(device)\n",
    "        print(\"Stage 2 model loaded with pre-trained weights from Stage 1.\")\n",
    "\n",
    "        # 5. 2단계 학습 시뮬레이션\n",
    "        # 일반적으로 vision_model은 동결하고 qformer와 projection layer만 학습\n",
    "        for param in model_stage2.vision_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        optimizer_s2 = torch.optim.AdamW(filter(lambda p: p.requires_grad, model_stage2.parameters()), lr=2e-5)\n",
    "\n",
    "        dummy_pixel_values_s2 = torch.randn(4, 3, 3, 224, 224).to(device)\n",
    "        dummy_prompt_ids_s2 = torch.randint(100, 30000, (4, 10)).to(device) # 프롬프트\n",
    "        dummy_labels_s2 = torch.randint(100, 30000, (4, 50)).to(device)   # 생성할 정답 텍스트\n",
    "\n",
    "        model_stage2.train()\n",
    "        output_s2 = model_stage2(\n",
    "            pixel_values=dummy_pixel_values_s2,\n",
    "            input_ids=dummy_prompt_ids_s2,\n",
    "            labels=dummy_labels_s2\n",
    "        )\n",
    "        loss_s2 = output_s2.loss\n",
    "        # loss_s2.backward() # 실제 학습 시 주석 해제\n",
    "        # optimizer_s2.step()\n",
    "        print(f\"Stage 2 Loss: {loss_s2.item():.4f}\")\n",
    "        \n",
    "        # 6. 학습된 2단계 모델로 추론(생성) 실행\n",
    "        model_stage2.eval()\n",
    "        print(\"\\nRunning generation...\")\n",
    "        generated_ids = model_stage2.generate(\n",
    "            pixel_values=dummy_pixel_values_s2[:1], # 첫 번째 이미지로 생성\n",
    "            input_ids=dummy_prompt_ids_s2[:1],      # 첫 번째 프롬프트 사용\n",
    "            max_length=30\n",
    "        )\n",
    "        print(f\"Generated token IDs: {generated_ids}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sur",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
