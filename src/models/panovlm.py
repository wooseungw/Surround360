from typing import Optional, Union, Tuple, Any, Dict, List
from transformers import PretrainedConfig, AutoConfig, AutoModel, PreTrainedModel, AutoModelForCausalLM
import transformers
import torch
import torch.nn as nn
from transformers import PreTrainedModel
from contextlib import nullcontext
import math
import logging

# íŠ¹ìˆ˜ í† í° ì¸ë±ìŠ¤ ì„¤ì •
IMAGE_TOKEN_INDEX = -200  # ì´ë¯¸ì§€ í† í° ì „ìš© ì¸ë±ìŠ¤
IGNORE_INDEX = -100  # ì†ì‹¤ ê³„ì‚° ë¬´ì‹œ ì¸ë±ìŠ¤

# íŠ¹ìˆ˜ í† í° ë¬¸ìì—´ ì„¤ì •
DEFAULT_IMAGE_TOKEN = "<image>"
DEFAULT_IMAGE_PATCH_TOKEN = "<im_patch>"
DEFAULT_IM_START_TOKEN = "<im_start>"
DEFAULT_IM_END_TOKEN = "<im_end>"

# VLM ëª¨ë¸ ë§¤í•‘ - ì–¸ì–´ ëª¨ë¸ ë° ì‹œê° ëª¨ë¸ ë¶„ë¦¬ë¥¼ ìœ„í•œ êµ¬ì„±
VLM_MAPPING = {
    # Gemma 3 ëª¨ë¸
    "google/gemma-3-4b-it": {
        "model_class": transformers.Gemma3ForConditionalGeneration,
        "vision_attr": None,  # ë…ë¦½ì ì¸ vision encoder ì‚¬ìš©
        "language_attr": "language_model"  # language_model ì†ì„±ì„ ì°¸ì¡°
    },
    "google/gemma-3-7b-it": {
        "model_class": transformers.Gemma3ForConditionalGeneration,
        "vision_attr": None,
        "language_attr": "language_model"
    },
    
    # Qwen ëª¨ë¸
    "Qwen/Qwen2.5-VL-3B-Instruct": {
        "model_class": transformers.Qwen2VLForConditionalGeneration,
        "vision_attr": None,  # ë…ë¦½ì ì¸ vision encoder ì‚¬ìš©
        "language_attr": "model"  # model ì†ì„±ì„ ì°¸ì¡°
    },
    "Qwen/Qwen2.5-VL-7B-Instruct": {
        "model_class": transformers.Qwen2VLForConditionalGeneration,
        "vision_attr": None,
        "language_attr": "model"
    },
    
    # Llama 3.1 ëª¨ë¸ ê³„ì—´
    "meta-llama/Meta-Llama-3.1-8B-Instruct": {
        "model_class": transformers.LlamaForCausalLM,
        "vision_attr": None,
        "language_attr": None  # ì „ì²´ ëª¨ë¸ì´ ì–¸ì–´ ëª¨ë¸
    },
    "meta-llama/Meta-Llama-3.1-70B-Instruct": {
        "model_class": transformers.LlamaForCausalLM,
        "vision_attr": None,
        "language_attr": None
    },
    
    # Mistral AI ìµœì‹  ëª¨ë¸
    "mistralai/Mistral-7B-Instruct-v0.3": {
        "model_class": transformers.MistralForCausalLM,
        "vision_attr": None,
        "language_attr": None
    },
}

# ë¹„ì „ ëª¨ë¸ ë§¤í•‘ (ë‹¤ì–‘í•œ ë¹„ì „ ì¸ì½”ë” ì§€ì›)
VISION_MAPPING = {
    # CLIP ê³„ì—´ ëª¨ë¸ë“¤
    "openai/clip-vit-large-patch14": AutoModel,  # CLIP ë¹„ì „ ì¸ì½”ë”
    "laion/CLIP-ViT-H-14-laion2B-s32B-b79K": AutoModel,  # LAION CLIP ë¹„ì „ ì¸ì½”ë”
    
    # DINO v2 ê³„ì—´ ëª¨ë¸ë“¤
    "facebook/dinov2-large": AutoModel,  # DINO v2 ë¹„ì „ ì¸ì½”ë”
    "facebook/dinov2-giant": AutoModel,  # DINO v2 Giant ë¹„ì „ ì¸ì½”ë”
    
    # SigLIP ëª¨ë¸ë“¤ (Google Researchì˜ í™•ì¥ëœ CLIP)
    "google/siglip-base-patch16": AutoModel,  # SigLIP base ëª¨ë¸
    "google/siglip-large-patch16": AutoModel,  # SigLIP large ëª¨ë¸
    "google/siglip-so400m-patch14-384": AutoModel,  # SigLIP SO400M ëª¨ë¸
    
    # EVA ëª¨ë¸ (MIM + CLIP)
    "BAAI/EVA-CLIP": AutoModel,  # EVA-CLIP
    
    # DINOv2+
    "facebook/dinov2-xlarge": AutoModel,  # DINO v2 XLarge
    
    # SAM (Segment Anything Model) ê´€ë ¨ ì¸ì½”ë”
    "facebook/sam-vit-huge": AutoModel,  # SAM ViT-H ëª¨ë¸ì˜ ì´ë¯¸ì§€ ì¸ì½”ë”
    
    # ë‚˜ë¨¸ì§€ëŠ” ê¸°ë³¸ AutoModelë¡œ ë¡œë“œ
}

def get_model_class(model_name: str):
    """í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ ì´ë¦„ì— ë”°ë¼ ì ì ˆí•œ ëª¨ë¸ í´ë˜ìŠ¤ ë°˜í™˜"""
    if model_name in VLM_MAPPING:
        return VLM_MAPPING[model_name]["model_class"]
    return transformers.AutoModelForCausalLM

def get_vision_model_class(model_name: str):
    """ë¹„ì „ ëª¨ë¸ ì´ë¦„ì— ë”°ë¼ ì ì ˆí•œ ëª¨ë¸ í´ë˜ìŠ¤ ë°˜í™˜"""
    if model_name in VISION_MAPPING:
        return VISION_MAPPING[model_name]
    return AutoModel

def extract_text_model(full_vlm: Any, model_name: str = None) -> Any:
    """
    ì „ì²´ VLM ì¸ìŠ¤í„´ìŠ¤(full_vlm)ë¡œë¶€í„° ìˆœìˆ˜ í…ìŠ¤íŠ¸ ìƒì„± LM ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜í•œë‹¤.
    
    Args:
        full_vlm: ì „ì²´ VLM ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
        model_name: ëª¨ë¸ ì´ë¦„ (ì„ íƒì , ì œê³µë˜ë©´ ë§¤í•‘ì„ ì‚¬ìš©í•´ ì§ì ‘ ì†ì„±ì— ì ‘ê·¼)
        
    Returns:
        ìˆœìˆ˜ í…ìŠ¤íŠ¸ ìƒì„± LM ë¶€ë¶„
    
    ìƒˆ VLMì„ ì¶”ê°€í•˜ë ¤ë©´ VLM_MAPPINGì— ë§¤í•‘ ì •ë³´ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.
    """
    # ëª¨ë¸ ì´ë¦„ì´ ì œê³µë˜ê³  ë§¤í•‘ì— ìˆìœ¼ë©´ ë°”ë¡œ ì ì ˆí•œ ì†ì„± ë°˜í™˜
    if model_name and model_name in VLM_MAPPING:
        attr_name = VLM_MAPPING[model_name]["language_attr"]
        if attr_name:
            return getattr(full_vlm, attr_name)
        return full_vlm
    
    # ê¸°ì¡´ ì¸ìŠ¤í„´ìŠ¤ ê¸°ë°˜ ë¡œì§ ìœ ì§€
    if isinstance(full_vlm, transformers.Gemma3ForConditionalGeneration):
        return full_vlm.language_model
    elif isinstance(full_vlm, transformers.Qwen2VLForConditionalGeneration):
        return full_vlm.model
    else:
        return full_vlm

class PanoVLM(PreTrainedModel):
    def __init__(self, config, *inputs, **kwargs):
        super().__init__(config, *inputs, **kwargs)
        self.config = config
        cache_dir = getattr(config, "cache_dir", "./.cache")
        
        # 1) Vision encoder ë¡œë“œ - ë‹¤ì–‘í•œ ë¹„ì „ ëª¨ë¸ ì§€ì›
        vision_cls = get_vision_model_class(config.vision_model_name_or_path)
        self.vision_model = vision_cls.from_pretrained(
            config.vision_model_name_or_path,
            config=config.vision_config,
            cache_dir=cache_dir
        )
        
        # 2) Language model ë¡œë“œ - ë‹¤ì–‘í•œ ì–¸ì–´ ëª¨ë¸ ì§€ì›
        lang_cls = get_model_class(config.language_model_name_or_path)
        
        # Gemma3 ëª¨ë¸ì„ ìœ„í•œ eager ì–´í…ì…˜ êµ¬í˜„ ì ìš©
        load_kwargs = {
            "config": config.language_config,
            "cache_dir": cache_dir
        }
        
        # Gemma3 ëª¨ë¸ì¸ ê²½ìš° eager ì–´í…ì…˜ êµ¬í˜„ìœ¼ë¡œ ì„¤ì •
        if "gemma-3" in config.language_model_name_or_path.lower():
            print("Gemma3 ëª¨ë¸ì— eager ì–´í…ì…˜ êµ¬í˜„ ì ìš©")
            load_kwargs["attn_implementation"] = "eager"
        
        full_vlm = lang_cls.from_pretrained(
            config.language_model_name_or_path,
            **load_kwargs
        )
        self.language_model = extract_text_model(full_vlm, config.language_model_name_or_path)

        # 3) Projector ì„¤ì • - ë¹„ì „ ì„ë² ë”©ì„ ì–¸ì–´ ëª¨ë¸ì— ë§ê²Œ íˆ¬ì˜
        if hasattr(config, "projector_config") and config.projector_config:
            from src.models.projector import build_vision_projector
            
            # í”„ë¡œì í„° êµ¬ì„± íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°
            # ë¹„ì „ ëª¨ë¸ì˜ ì°¨ì› ê°€ì ¸ì˜¤ê¸°
            if hasattr(self.vision_model.config, 'hidden_size'):
                vision_dim = self.vision_model.config.hidden_size
            elif hasattr(self.vision_model.config, 'embed_dim'):
                vision_dim = self.vision_model.config.embed_dim  # CLIP ëª¨ë¸
            else:
                vision_dim = 384  # DinoV2-small ê¸°ë³¸ê°’
                
            # ì–¸ì–´ ëª¨ë¸ì˜ ì°¨ì› ê°€ì ¸ì˜¤ê¸°
            if hasattr(self.language_model.config, 'hidden_size'):
                lang_dim = self.language_model.config.hidden_size
            elif hasattr(self.language_model.config, 'model_dim'):
                lang_dim = self.language_model.config.model_dim  # Gemma-3 ëª¨ë¸
            elif hasattr(self.language_model.config, 'hidden_dim'):
                lang_dim = self.language_model.config.hidden_dim
            elif hasattr(self.language_model.config, 'd_model'):
                lang_dim = self.language_model.config.d_model  # ì¼ë¶€ transformer ëª¨ë¸
            else:
                lang_dim = 2560  # ê¸°ë³¸ê°’ (Gemma-3-4B)
                
            in_dim = getattr(config.projector_config, "in_features", vision_dim)
            out_dim = getattr(config.projector_config, "out_features", lang_dim)
            projector_type = getattr(config.projector_config, "type", "mlp2x_gelu")
            
            # í”„ë¡œì í„° ìƒì„± (mlp2x_geluëŠ” ì¶”ê°€ ì„¤ì •ì´ í•„ìš” ì—†ìŒ)
            self.projector = build_vision_projector(
                d_v=in_dim, 
                d_l=out_dim, 
                projector_type=projector_type,
                vision_cfg=self.vision_model.config if projector_type == "pooler" else None
            )
        else:
            self.projector = None

        # 4) ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì„¤ì •
        self.use_fp16 = getattr(config, "use_fp16", True)
        self.gradient_checkpointing = getattr(config, "gradient_checkpointing", False)
        
        if self.gradient_checkpointing:
            self.vision_model.gradient_checkpointing_enable()
            if hasattr(self.language_model, "gradient_checkpointing_enable"):
                self.language_model.gradient_checkpointing_enable()
                
        # 5) ì´ë¯¸ì§€ í† í¬ë‚˜ì´ì € ì„¤ì •
        self.image_token_index = getattr(config, "image_token_index", IMAGE_TOKEN_INDEX)
        self.tokenizer = getattr(config, "tokenizer", None)
        
        # 6) ìƒˆë¡œ ì •ì˜ëœ ë ˆì´ì–´ ì´ˆê¸°í™”
        self.init_weights()
    
    def preprocess_image_tokens(self, input_ids, image_token_id=None):
        """
        í…ìŠ¤íŠ¸ ì…ë ¥ì—ì„œ ì´ë¯¸ì§€ í† í°ì„ íŠ¹ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
        """
        if image_token_id is None and hasattr(self, "tokenizer") and self.tokenizer is not None:
            image_token_id = self.tokenizer.convert_tokens_to_ids(DEFAULT_IMAGE_TOKEN)
        
        if image_token_id is None:
            return input_ids
        
        # ì…ë ¥ ID ë³µì‚¬ë³¸ ìƒì„±
        processed_input_ids = input_ids.clone()
        
        # ì´ë¯¸ì§€ í† í°ì„ íŠ¹ìˆ˜ ì¸ë±ìŠ¤ë¡œ êµì²´
        mask = (processed_input_ids == image_token_id)
        processed_input_ids[mask] = self.image_token_index
        
        return processed_input_ids
    
    def _replace_image_tokens_with_features(
        self, 
        input_ids, 
        labels=None, 
        attention_mask=None, 
        vision_embeds=None,
        ignore_index=IGNORE_INDEX
    ):
        """
        ì…ë ¥ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ì˜ ì´ë¯¸ì§€ í† í°ì„ ë¹„ì „ ì„ë² ë”©ìœ¼ë¡œ ëŒ€ì²´
        
        Args:
            input_ids: ì…ë ¥ í† í° ID (B, L)
            labels: ë ˆì´ë¸” í† í° ID (B, L)
            attention_mask: ì–´í…ì…˜ ë§ˆìŠ¤í¬ (B, L)
            vision_embeds: ë¹„ì „ ì„ë² ë”© (B, P, S, D) ë˜ëŠ” (B, S, D)
            ignore_index: ì†ì‹¤ ê³„ì‚° ë¬´ì‹œ ì¸ë±ìŠ¤
            
        Returns:
            combined_embeds: ê²°í•©ëœ ì„ë² ë”©
            combined_labels: ê²°í•©ëœ ë ˆì´ë¸”
            combined_mask: ê²°í•©ëœ ì–´í…ì…˜ ë§ˆìŠ¤í¬
            position_ids: ìœ„ì¹˜ ì¸ë±ìŠ¤
        """
        device = input_ids.device
        batch_size = input_ids.size(0)
        
        # ì–´í…ì…˜ ë§ˆìŠ¤í¬ê°€ ì—†ìœ¼ë©´ íŒ¨ë”© í† í°ì„ ì œì™¸í•œ ëª¨ë“  í† í°ì— ë§ˆìŠ¤í¬ ì ìš©
        if attention_mask is None and hasattr(self, "tokenizer") and self.tokenizer is not None:
            attention_mask = (input_ids != self.tokenizer.pad_token_id)
        elif attention_mask is None:
            attention_mask = torch.ones_like(input_ids).bool()
        else:
            attention_mask = attention_mask.bool()
            
        # ë ˆì´ë¸”ì´ ì—†ìœ¼ë©´ ì…ë ¥ IDì™€ ë™ì¼í•˜ê²Œ ì„¤ì •
        if labels is None:
            labels = input_ids.clone()
        
        # ì…ë ¥ ì„ë² ë”© ê°€ì ¸ì˜¤ê¸°
        embed_tokens_fn = self.language_model.get_input_embeddings()
        
        # íŒ¨ë”© ì œê±°í•˜ê³  ì‹¤ì œ í† í°ë§Œ ì²˜ë¦¬
        ids_list = [ids[mask] for ids, mask in zip(input_ids, attention_mask)]
        lbl_list = [lbl[mask] for lbl, mask in zip(labels, attention_mask)]
        
        # ë°°ì¹˜ë³„ ì²˜ë¦¬
        seq_embeds, seq_labels = [], []
        is_panorama = len(vision_embeds.shape) == 4  # (B, P, S, D) í˜•íƒœì¸ì§€ í™•ì¸
        
        for b in range(batch_size):
            ids = ids_list[b]
            lbls = lbl_list[b]
            
            # ë¹„ì „ ì„ë² ë”© ê°€ì ¸ì˜¤ê¸° - ë°°ì¹˜ë³„ë¡œ í•´ë‹¹í•˜ëŠ” ì„ë² ë”© ì‚¬ìš©
            if is_panorama:
                # (B, P, S, D) -> (P*S, D)ë¡œ ë³€í™˜
                B, P, S, D = vision_embeds.shape
                vis_emb = vision_embeds[b].reshape(-1, D)
            else:
                # (B, S, D) í˜•íƒœ
                vis_emb = vision_embeds[b]
            
            if vis_emb.dim() == 1:
                vis_emb = vis_emb.unsqueeze(0)
                
            # ì´ë¯¸ì§€ í† í° ìœ„ì¹˜ ì°¾ê¸°
            img_pos = (ids == self.image_token_index).nonzero(as_tuple=False).flatten()
            if img_pos.numel() == 0:
                # ì´ë¯¸ì§€ í† í°ì´ ì—†ìœ¼ë©´ ê·¸ëƒ¥ í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬
                seq_embeds.append(embed_tokens_fn(ids))
                seq_labels.append(lbls)
                continue
                
            # ë¶„í•  ì§€ì  ì„¤ì • (ì´ë¯¸ì§€ í† í° ìœ„ì¹˜ ê¸°ì¤€)
            split_pts = torch.cat([torch.tensor([-1], device=device), img_pos, torch.tensor([ids.size(0)], device=device)])
            seg_emb, seg_lbl = [], []
            
            # ì„¸ê·¸ë¨¼íŠ¸ ë³„ë¡œ ì²˜ë¦¬
            for i in range(split_pts.numel() - 1):
                s = split_pts[i] + 1
                e = split_pts[i + 1]
                txt_ids = ids[s:e]
                txt_lbl = lbls[s:e]
                
                # í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸ ì„ë² ë”©
                if txt_ids.numel() > 0:
                    txt_emb = embed_tokens_fn(txt_ids)
                else:
                    # ë¹ˆ ì„¸ê·¸ë¨¼íŠ¸
                    txt_emb = vis_emb[:0]
                
                seg_emb.append(txt_emb)
                seg_lbl.append(txt_lbl)
                
                # ì´ë¯¸ì§€ í† í° ìœ„ì¹˜ì— ë¹„ì „ ì„ë² ë”© ì‚½ì…
                if i < img_pos.numel():
                    seg_emb.append(vis_emb)
                    # ì´ë¯¸ì§€ í† í°ë„ í•™ìŠµì— í¬í•¨ (ì´ë¯¸ì§€ í† í° IDë¥¼ ë ˆì´ë¸”ë¡œ ì‚¬ìš©)
                    seg_lbl.append(torch.full((vis_emb.size(0),), self.image_token_index, dtype=lbls.dtype, device=device))
            
            # ì„¸ê·¸ë¨¼íŠ¸ ê²°í•©
            seq_embeds.append(torch.cat(seg_emb, dim=0))
            seq_labels.append(torch.cat(seg_lbl, dim=0))
        
        # ìµœëŒ€ ê¸¸ì´ íŒ¨ë”© ì²˜ë¦¬
        max_length = max(e.size(0) for e in seq_embeds)
        embed_dim = seq_embeds[0].size(1)
        
        # íŒ¨ë”©ì„ ìœ„í•œ ì´ˆê¸°í™”
        pad_embeds = []
        pad_labels = torch.full((batch_size, max_length), ignore_index, dtype=labels.dtype, device=device)
        pad_mask = torch.zeros((batch_size, max_length), dtype=torch.long, device=device)
        pos_ids = torch.zeros((batch_size, max_length), dtype=torch.long, device=device)
        
        # íŒ¨ë”© í•¨ìˆ˜
        pad_emb = lambda n: torch.zeros((n, embed_dim), dtype=seq_embeds[0].dtype, device=device)
        
        # ê° ì‹œí€€ìŠ¤ë¥¼ ìµœëŒ€ ê¸¸ì´ì— ë§ê²Œ íŒ¨ë”©
        padding_side = "right"  # ê¸°ë³¸ê°’ìœ¼ë¡œ ì˜¤ë¥¸ìª½ íŒ¨ë”©
        if hasattr(self, "tokenizer") and hasattr(self.tokenizer, "padding_side"):
            padding_side = self.tokenizer.padding_side
            
        for i, (emb, lab) in enumerate(zip(seq_embeds, seq_labels)):
            cur_len = emb.size(0)
            if padding_side == "left":
                pad = pad_emb(max_length - cur_len)
                emb = torch.cat([pad, emb], dim=0)
                pad_labels[i, -cur_len:] = lab
                pad_mask[i, -cur_len:] = 1
                pos_ids[i, -cur_len:] = torch.arange(cur_len, device=device)
            else:  # right padding
                pad = pad_emb(max_length - cur_len)
                emb = torch.cat([emb, pad], dim=0)
                pad_labels[i, :cur_len] = lab
                pad_mask[i, :cur_len] = 1
                pos_ids[i, :cur_len] = torch.arange(cur_len, device=device)
            
            pad_embeds.append(emb)
        
        # ìµœì¢… ê²°í•© ì„ë² ë”©
        combined_embeds = torch.stack(pad_embeds, dim=0)
        return combined_embeds, pad_labels, pad_mask, pos_ids

    def _combine_embeddings(
        self,
        pixel_values,
        input_ids,
        attention_mask=None,
        labels=None,
        interpolate_pos_encoding=False
    ):
        """
        ë¹„ì „ ì„ë² ë”©ê³¼ í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ê²°í•©í•˜ëŠ” í¬ê´„ì  ë©”ì„œë“œ
        
        Args:
            pixel_values: ì´ë¯¸ì§€ í”½ì…€ ê°’
            input_ids: ì…ë ¥ í† í° ID
            attention_mask: ì–´í…ì…˜ ë§ˆìŠ¤í¬
            labels: ë ˆì´ë¸” í† í° ID
            interpolate_pos_encoding: ìœ„ì¹˜ ì¸ì½”ë”© ë³´ê°„ ì—¬ë¶€
            
        Returns:
            combined_embeds: ê²°í•©ëœ ì„ë² ë”©
            combined_labels: ê²°í•©ëœ ë ˆì´ë¸”
            combined_mask: ê²°í•©ëœ ì–´í…ì…˜ ë§ˆìŠ¤í¬
            position_ids: ìœ„ì¹˜ ì¸ë±ìŠ¤
        """
        # 1. ì´ë¯¸ì§€ í† í° ì „ì²˜ë¦¬
        if hasattr(self, "tokenizer") and self.tokenizer is not None:
            image_token_id = self.tokenizer.convert_tokens_to_ids(DEFAULT_IMAGE_TOKEN)
            processed_input_ids = self.preprocess_image_tokens(input_ids, image_token_id)
            if labels is not None:
                processed_labels = self.preprocess_image_tokens(labels, image_token_id)
            else:
                processed_labels = None
        else:
            processed_input_ids = input_ids
            processed_labels = labels
        
        # 2. ë¹„ì „ ëª¨ë¸ ì²˜ë¦¬
        # ì…ë ¥ í…ì„œ í˜•íƒœ í™•ì¸ ë° ì¡°ì •
        pixel_values_reshaped, batch_size, P, is_panorama = self.process_input(pixel_values)
        
        # Vision encoder ì²˜ë¦¬
        vision_kwargs = {"return_dict": True}
        if "clip" in str(self.vision_model.__class__).lower() and interpolate_pos_encoding:
            vision_kwargs["interpolate_pos_encoding"] = interpolate_pos_encoding
        
        vision_outputs = self.vision_model(
            pixel_values=pixel_values_reshaped,
            **vision_kwargs
        )
        
        # Vision ì¶œë ¥ì—ì„œ íŠ¹ì„± ì¶”ì¶œ
        if hasattr(vision_outputs, 'last_hidden_state'):
            vision_embeds = vision_outputs.last_hidden_state  # transformer ê¸°ë°˜ ì¶œë ¥
        elif isinstance(vision_outputs, tuple) and len(vision_outputs) > 0:
            vision_embeds = vision_outputs[0]  # íŠœí”Œ í˜•íƒœ ì¶œë ¥ ì²˜ë¦¬
        else:
            vision_embeds = vision_outputs  # ê¸°íƒ€ ì¶œë ¥
        
        # íŒŒë…¸ë¼ë§ˆ ì´ë¯¸ì§€ì¸ ê²½ìš° reshape
        if is_panorama:
            _, S, D = vision_embeds.shape
            vision_embeds = vision_embeds.view(batch_size, P, S, D)
        
        # 3. Projector ì ìš©
        if self.projector is not None:
            if is_panorama:
                B, P, S, D = vision_embeds.shape
                vision_embeds = vision_embeds.view(-1, D)  # ë§ˆì§€ë§‰ ì°¨ì›ë§Œ íˆ¬ì˜
                vision_embeds = self.projector(vision_embeds)
                vision_embeds = vision_embeds.view(B, P, S, -1)  # ì›ë˜ í˜•íƒœë¡œ ë³µì›
            else:
                vision_embeds = self.projector(vision_embeds)
        
        # 4. í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ì„ë² ë”© ê²°í•©
        return self._replace_image_tokens_with_features(
            input_ids=processed_input_ids,
            labels=processed_labels,
            attention_mask=attention_mask,
            vision_embeds=vision_embeds
        )
    
    def process_input(self, pixel_values):
        """
        ì…ë ¥ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” í—¬í¼ ë©”ì„œë“œ
        
        Args:
            pixel_values: ì…ë ¥ ì´ë¯¸ì§€ í”½ì…€ ê°’
            
        Returns:
            pixel_values_reshaped: ì¬êµ¬ì„±ëœ í”½ì…€ ê°’
            batch_size: ë°°ì¹˜ í¬ê¸°
            P: íŒ¨ì¹˜/ë·°ì˜ ìˆ˜
            is_panorama: íŒŒë…¸ë¼ë§ˆ ì´ë¯¸ì§€ ì—¬ë¶€
        """
        # ì…ë ¥ í…ì„œ í˜•íƒœ í™•ì¸ ë° ì¡°ì •
        if len(pixel_values.shape) == 5:  # [B, P, C, H, W]
            B, P, C, H, W = pixel_values.shape
            return pixel_values.view(B * P, C, H, W), B, P, True
        elif len(pixel_values.shape) == 4:  # [B, C, H, W]
            B = pixel_values.shape[0]
            return pixel_values, B, 1, False
        else:
            raise ValueError(f"Unexpected pixel_values shape: {pixel_values.shape}. " 
                          f"Expected [B, P, C, H, W] or [B, C, H, W]")
            
    def forward(
        self,
        pixel_values: torch.FloatTensor,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.LongTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        interpolate_pos_encoding: bool = False,
        **kwargs
        ):
        """
        Forward pass for the PanoVLM model.
        
        Args:
            pixel_values (torch.FloatTensor): ë¹„ì „ ëª¨ë¸ì„ ìœ„í•œ ì…ë ¥ í”½ì…€ ê°’
            input_ids (Optional[torch.LongTensor]): ì–¸ì–´ ëª¨ë¸ì„ ìœ„í•œ ì…ë ¥ í† í° ID
            attention_mask (Optional[torch.LongTensor]): ì–¸ì–´ ëª¨ë¸ì„ ìœ„í•œ ì–´í…ì…˜ ë§ˆìŠ¤í¬
            labels (Optional[torch.LongTensor]): ì–¸ì–´ ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ë ˆì´ë¸”
            interpolate_pos_encoding (bool): ìœ„ì¹˜ ì¸ì½”ë”© ë³´ê°„ ì—¬ë¶€
            **kwargs: ì¶”ê°€ ì¸ì
            
        Returns:
            torch.FloatTensor: ì–¸ì–´ ëª¨ë¸ì˜ ì¶œë ¥ ë¡œì§“
        """
        # FP16 ìë™ ë³€í™˜ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
        autocast_ctx = torch.cuda.amp.autocast() if self.use_fp16 and torch.cuda.is_available() else nullcontext()
        
        with autocast_ctx:
            # ë¹„ì „ ì„ë² ë”©ê³¼ í…ìŠ¤íŠ¸ ì„ë² ë”© ê²°í•©
            inputs_embeds, combined_labels, combined_mask, position_ids = self._combine_embeddings(
                pixel_values=pixel_values,
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels,
                interpolate_pos_encoding=interpolate_pos_encoding
            )
            
            # ëª…ì‹œì ìœ¼ë¡œ requires_grad=True ì„¤ì •í•˜ì—¬ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë³´ì¥
            if self.training and not inputs_embeds.requires_grad:
                inputs_embeds.requires_grad_(True)
            
            # LLM ëª¨ë¸ì— ì „ë‹¬í•  ì¸ì êµ¬ì„±
            model_kwargs = {
                "attention_mask": combined_mask if attention_mask is not None else None,
                "position_ids": position_ids,
                "return_dict": True
            }
            
            # í•™ìŠµ ëª¨ë“œì—ì„œëŠ” ë ˆì´ë¸” ì „ë‹¬
            if labels is not None:
                model_kwargs["labels"] = combined_labels
                
            # ì–¸ì–´ ëª¨ë¸ í˜¸ì¶œ
            return self.language_model(
                inputs_embeds=inputs_embeds,
                **model_kwargs
            )
    

    @torch.no_grad()
    def generate(
        self,
        pixel_values: torch.FloatTensor,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.LongTensor] = None,
        interpolate_pos_encoding: bool = False,
        **generate_kwargs,
    ):
        """
        ì…ë ¥ í”½ì…€ ê°’ê³¼ ì„ íƒì  í…ìŠ¤íŠ¸ ì…ë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            pixel_values (torch.FloatTensor): ë¹„ì „ ëª¨ë¸ì„ ìœ„í•œ ì…ë ¥ í”½ì…€ ê°’
            input_ids (Optional[torch.LongTensor]): ì–¸ì–´ ëª¨ë¸ì„ ìœ„í•œ ì…ë ¥ í† í° ID
            attention_mask (Optional[torch.LongTensor]): ì–¸ì–´ ëª¨ë¸ì„ ìœ„í•œ ì–´í…ì…˜ ë§ˆìŠ¤í¬
            interpolate_pos_encoding (bool): ìœ„ì¹˜ ì¸ì½”ë”© ë³´ê°„ ì—¬ë¶€
            **generate_kwargs: ìƒì„±ì„ ìœ„í•œ ì¶”ê°€ ì¸ì
            
        Returns:
            torch.LongTensor: ìƒì„±ëœ í† í° ID
        """
        # FP16 ìë™ ë³€í™˜ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
        autocast_ctx = torch.cuda.amp.autocast() if self.use_fp16 and torch.cuda.is_available() else nullcontext()
        
        with autocast_ctx:
            # ë¹„ì „ ì„ë² ë”©ê³¼ í…ìŠ¤íŠ¸ ì„ë² ë”© ê²°í•©
            inputs_embeds, _, combined_mask, position_ids = self._combine_embeddings(
                pixel_values=pixel_values,
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=None,
                interpolate_pos_encoding=interpolate_pos_encoding
            )
            
            # ìƒì„±ì— í•„ìš”í•œ model_kwargs êµ¬ì„±
            model_kwargs = {
                "inputs_embeds": inputs_embeds,
                "attention_mask": combined_mask if attention_mask is not None else None,
                "position_ids": position_ids,
            }
            
            # ì–¸ì–´ ëª¨ë¸ì˜ generate ë©”ì„œë“œ í˜¸ì¶œ
            return self.language_model.generate(
                input_ids=None,  # inputs_embedsë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ input_idsëŠ” None
                **model_kwargs,
                **generate_kwargs
            )
    
    def gradient_checkpointing_enable(self, **kwargs):
        """
        Enable gradient checkpointing for the model.
        TrainerëŠ” ëª¨ë¸ì— ì´ ë©”ì„œë“œê°€ ìˆì–´ì•¼ gradient_checkpointing=True ì„¤ì •ì„ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        
        gradient_checkpointingì˜ í™œì„±í™” ìƒíƒœë¥¼ ê°œì„ í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤.
        í”„ë¡œì í„° ë ˆì´ì–´ë„ checkpointingì´ ê°€ëŠ¥í•œ ê²½ìš° í™œì„±í™”í•©ë‹ˆë‹¤.
        """
        print("\n===== Gradient Checkpointing í™œì„±í™” =====")
        
        # 1. Vision ëª¨ë¸ì— gradient checkpointing í™œì„±í™”
        try:
            if hasattr(self.vision_model, "gradient_checkpointing_enable"):
                self.vision_model.gradient_checkpointing_enable()
                print("âœ… Vision ëª¨ë¸: gradient_checkpointing_enable ë©”ì„œë“œ ì‚¬ìš© ì„±ê³µ")
            elif hasattr(self.vision_model, "config") and hasattr(self.vision_model.config, "gradient_checkpointing"):
                self.vision_model.config.gradient_checkpointing = True
                print("âœ… Vision ëª¨ë¸: config.gradient_checkpointing ì„¤ì • ì„±ê³µ")
            else:
                # ëŒ€ì²´ ë°©ë²•: ì§ì ‘ ì†ì„± ì„¤ì •
                self.vision_model.gradient_checkpointing = True
                print("âœ… Vision ëª¨ë¸: ì§ì ‘ ì†ì„± ì„¤ì • ì„±ê³µ")
                
            # Vision ëª¨ë¸ íŒŒë¼ë¯¸í„° ë™ê²° í™•ì¸
            if not any(p.requires_grad for p in self.vision_model.parameters()):
                print("ğŸ“ ì°¸ê³ : Vision ëª¨ë¸ì€ ë™ê²° ìƒíƒœë¼ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸ Vision ëª¨ë¸ gradient checkpointing í™œì„±í™” ì‹¤íŒ¨: {str(e)}")
            
        # 2. Language ëª¨ë¸ì— gradient checkpointing í™œì„±í™” 
        try:
            if hasattr(self.language_model, "gradient_checkpointing_enable"):
                self.language_model.gradient_checkpointing_enable()
                print("âœ… Language ëª¨ë¸: gradient_checkpointing_enable ë©”ì„œë“œ ì‚¬ìš© ì„±ê³µ")
            elif hasattr(self.language_model, "config") and hasattr(self.language_model.config, "gradient_checkpointing"):
                self.language_model.config.gradient_checkpointing = True
                print("âœ… Language ëª¨ë¸: config.gradient_checkpointing ì„¤ì • ì„±ê³µ")
            else:
                # ëŒ€ì²´ ë°©ë²•: ì§ì ‘ ì†ì„± ì„¤ì •
                self.language_model.gradient_checkpointing = True
                print("âœ… Language ëª¨ë¸: ì§ì ‘ ì†ì„± ì„¤ì • ì„±ê³µ")
                
            # Language ëª¨ë¸ íŒŒë¼ë¯¸í„° ë™ê²° í™•ì¸
            if not any(p.requires_grad for p in self.language_model.parameters()):
                print("ğŸ“ ì°¸ê³ : Language ëª¨ë¸ì€ ë™ê²° ìƒíƒœë¼ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸ Language ëª¨ë¸ gradient checkpointing í™œì„±í™” ì‹¤íŒ¨: {str(e)}")
        
        # 3. Projector ëª¨ë“ˆì— gradient checkpointing ì ìš© (ê°€ëŠ¥í•œ ê²½ìš°)
        if self.projector is not None:
            try:
                # Linear ë ˆì´ì–´ì™€ ê°™ì€ ë‹¨ì¼ ì—°ì‚°ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì ìš©
                if isinstance(self.projector, nn.Sequential) and len(list(self.projector.children())) > 1:
                    # checkpoint í•¨ìˆ˜ë¥¼ ì§ì ‘ ì ìš©í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë©í•‘
                    original_forward = self.projector.forward
                    
                    def checkpointed_forward(*args, **kwargs):
                        if torch.is_grad_enabled() and any(p.requires_grad for p in self.projector.parameters()):
                            from torch.utils.checkpoint import checkpoint
                            return checkpoint(original_forward, *args, **kwargs)
                        else:
                            return original_forward(*args, **kwargs)
                    
                    # í”„ë¡œì í„°ì˜ forward ë©”ì„œë“œ êµì²´
                    self.projector.forward = checkpointed_forward
                    print("âœ… Projector: ì»¤ìŠ¤í…€ gradient checkpointing ì ìš© ì„±ê³µ")
                    
                    # Projector ëª¨ë¸ í•™ìŠµ ìƒíƒœ í™•ì¸
                    if any(p.requires_grad for p in self.projector.parameters()):
                        print("ğŸ“ ProjectorëŠ” í•™ìŠµ ìƒíƒœì´ë¯€ë¡œ gradient checkpointingì´ ì ìš©ë©ë‹ˆë‹¤.")
                else:
                    print("ğŸ“ ProjectorëŠ” ë‹¨ìˆœ êµ¬ì¡°ë¼ gradient checkpointingì´ ë¶ˆí•„ìš”í•©ë‹ˆë‹¤.")
            except Exception as e:
                print(f"âš ï¸ Projector gradient checkpointing ì„¤ì • ì‹¤íŒ¨: {str(e)}")
            
        # 4. ìºì‹œ ì‚¬ìš©ì„ ë¹„í™œì„±í™”í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
        if hasattr(self.language_model, "config") and hasattr(self.language_model.config, "use_cache"):
            self.language_model.config.use_cache = False
            print("âœ… Language ëª¨ë¸ use_cache ë¹„í™œì„±í™”ë¨")
        
        # 5. ê¸°íƒ€ ì‚¬ìš©ì ì œê³µ kwargs ì²˜ë¦¬
        for key, value in kwargs.items():
            print(f"ğŸ“ ì¶”ê°€ ì„¤ì •: {key}={value}")
        
        self.gradient_checkpointing = True
        print("âœ… ëª¨ë¸ ì „ì²´ gradient_checkpointing í™œì„±í™” ì™„ë£Œ")
        print("========================================\n")

    def gradient_checkpointing_disable(self):
        """
        Disable gradient checkpointing for the model.
        
        ëª¨ë“  ì»´í¬ë„ŒíŠ¸ì˜ gradient checkpointingì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.
        """
        print("\n===== Gradient Checkpointing ë¹„í™œì„±í™” =====")
        
        # 1. Vision ëª¨ë¸ gradient checkpointing ë¹„í™œì„±í™”
        try:
            if hasattr(self.vision_model, "gradient_checkpointing_disable"):
                self.vision_model.gradient_checkpointing_disable()
                print("âœ… Vision ëª¨ë¸: gradient_checkpointing_disable ë©”ì„œë“œ í˜¸ì¶œ ì„±ê³µ")
            elif hasattr(self.vision_model, "config") and hasattr(self.vision_model.config, "gradient_checkpointing"):
                self.vision_model.config.gradient_checkpointing = False
                print("âœ… Vision ëª¨ë¸: config.gradient_checkpointing ë¹„í™œì„±í™”")
            elif hasattr(self.vision_model, "gradient_checkpointing"):
                self.vision_model.gradient_checkpointing = False
                print("âœ… Vision ëª¨ë¸: ì§ì ‘ ì†ì„± ë¹„í™œì„±í™”")
        except Exception as e:
            print(f"âš ï¸ Vision ëª¨ë¸ gradient checkpointing ë¹„í™œì„±í™” ì‹¤íŒ¨: {str(e)}")
        
        # 2. Language ëª¨ë¸ gradient checkpointing ë¹„í™œì„±í™”
        try:
            if hasattr(self.language_model, "gradient_checkpointing_disable"):
                self.language_model.gradient_checkpointing_disable()
                print("âœ… Language ëª¨ë¸: gradient_checkpointing_disable ë©”ì„œë“œ í˜¸ì¶œ ì„±ê³µ")
            elif hasattr(self.language_model, "config") and hasattr(self.language_model.config, "gradient_checkpointing"):
                self.language_model.config.gradient_checkpointing = False
                print("âœ… Language ëª¨ë¸: config.gradient_checkpointing ë¹„í™œì„±í™”")
            elif hasattr(self.language_model, "gradient_checkpointing"):
                self.language_model.gradient_checkpointing = False
                print("âœ… Language ëª¨ë¸: ì§ì ‘ ì†ì„± ë¹„í™œì„±í™”")
        except Exception as e:
            print(f"âš ï¸ Language ëª¨ë¸ gradient checkpointing ë¹„í™œì„±í™” ì‹¤íŒ¨: {str(e)}")
        
        # 3. Projector ëª¨ë“ˆì— ì ìš©ëœ gradient checkpointing ì›ë³µ (í•„ìš”í•œ ê²½ìš°)
        if self.projector is not None and isinstance(self.projector, nn.Sequential):
            try:
                # ì›ë˜ forward ë©”ì„œë“œê°€ ë©í•‘ë˜ì—ˆë‹¤ë©´ ì›ë˜ ë©”ì„œë“œë¡œ ë³µêµ¬
                if hasattr(self.projector, "_original_forward"):
                    self.projector.forward = self.projector._original_forward
                    delattr(self.projector, "_original_forward")
                    print("âœ… Projector: ì›ë˜ forward ë©”ì„œë“œë¡œ ë³µêµ¬ë¨")
            except Exception as e:
                print(f"âš ï¸ Projector gradient checkpointing ë¹„í™œì„±í™” ì‹¤íŒ¨: {str(e)}")
        
        # 4. ìºì‹œ ì‚¬ìš© ì¬í™œì„±í™”
        if hasattr(self.language_model, "config") and hasattr(self.language_model.config, "use_cache"):
            self.language_model.config.use_cache = True
            print("âœ… Language ëª¨ë¸ use_cache ì¬í™œì„±í™”ë¨")
        
        self.gradient_checkpointing = False
        print("âœ… ëª¨ë¸ ì „ì²´ gradient_checkpointing ë¹„í™œì„±í™” ì™„ë£Œ")
        print("==========================================\n")
        
    def set_tokenizer(self, tokenizer):
        """
        ëª¨ë¸ì— í† í¬ë‚˜ì´ì €ë¥¼ ì„¤ì •
        
        Args:
            tokenizer: í—ˆê¹…í˜ì´ìŠ¤ í† í¬ë‚˜ì´ì € ì¸ìŠ¤í„´ìŠ¤
        """
        self.tokenizer = tokenizer
        
        # íŠ¹ìˆ˜ í† í° ë“±ë¡ í™•ì¸ ë° ì²˜ë¦¬
        special_tokens = {
            "additional_special_tokens": [
                DEFAULT_IMAGE_TOKEN,
                DEFAULT_IMAGE_PATCH_TOKEN,
                DEFAULT_IM_START_TOKEN,
                DEFAULT_IM_END_TOKEN,
            ]
        }
        
        # í† í¬ë‚˜ì´ì €ì— íŠ¹ìˆ˜ í† í° ì¶”ê°€
        num_added = 0
        if hasattr(tokenizer, "add_special_tokens"):
            num_added = tokenizer.add_special_tokens(special_tokens)
            
        # ì„ë² ë”© í¬ê¸° ì¡°ì •
        if num_added > 0 and hasattr(self.language_model, "resize_token_embeddings"):
            self.language_model.resize_token_embeddings(len(tokenizer))
            
        # íŒ¨ë”© ê´€ë ¨ ì„¤ì •
        if not hasattr(tokenizer, "pad_token") or tokenizer.pad_token is None:
            if hasattr(tokenizer, "eos_token") and tokenizer.eos_token is not None:
                tokenizer.pad_token = tokenizer.eos_token
                
        return tokenizer