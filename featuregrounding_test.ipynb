{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b59a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do not Crop, Image size: (224, 224)\n",
      "==Input sequence==\n",
      "tensor([ 7201,  2192,  1347,   291,   269,   262,   266, 14455,  1033,   262,\n",
      "          266,   613, 10195,   268,   262,   266,  1408,  2965,   267,   262,\n",
      "          266,   741,   354,   280,   613,  7193,   262,   266,  2498,   263,\n",
      "          693,  2203,  2177,   289,   262,   266,  1226,  1882,   691,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1])\n",
      "query street answer this is a sidewalk near a large intersection in a commercial district of a city there are large trucks a bus and several cars waiting at a red traffic light</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "==Attention mask==\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'attention_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 145\u001b[39m\n\u001b[32m    130\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m torch.stack(patches, dim=\u001b[32m0\u001b[39m).unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m    133\u001b[39m dataset = QuIC360Dataset(\n\u001b[32m    134\u001b[39m     csv_file=\u001b[33m\"\u001b[39m\u001b[33mdata/quic360/downtest.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    135\u001b[39m     processor=AutoProcessor.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mgoogle/siglip-base-patch16-224\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m    142\u001b[39m     transform=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    143\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m inputs = \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow_image_grid\u001b[39m(images, ncols=\u001b[32m4\u001b[39m):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 80\u001b[39m, in \u001b[36mQuIC360Dataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m.processor.tokenizer.decode(inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[32m     79\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m==Attention mask==\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mattention_mask\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[32m0\u001b[39m])\n\u001b[32m     81\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m==Labels==\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     82\u001b[39m \u001b[38;5;28mprint\u001b[39m(labels[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/sur/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:271\u001b[39m, in \u001b[36mBatchEncoding.__getitem__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    261\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    262\u001b[39m \u001b[33;03mIf the key is a string, returns the value of the dict associated to `key` ('input_ids', 'attention_mask',\u001b[39;00m\n\u001b[32m    263\u001b[39m \u001b[33;03metc.).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    268\u001b[39m \u001b[33;03mwith the constraint of slice.\u001b[39;00m\n\u001b[32m    269\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._encodings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    273\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._encodings[item]\n",
      "\u001b[31mKeyError\u001b[39m: 'attention_mask'"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Union, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoProcessor\n",
    "from PIL import Image\n",
    "from py360convert import e2p\n",
    "IGNORE_INDEX = -100\n",
    "\n",
    "class QuIC360Dataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 csv_file: str,\n",
    "                 processor: AutoProcessor,\n",
    "                 max_length: Optional[int] = None,\n",
    "                 split: str = \"train\",\n",
    "                 do_crop: bool = False,\n",
    "                 fov: Optional[float] = None,\n",
    "                 overlap_ratio: Optional[float] = None,\n",
    "                 image_size: list = [224,224],\n",
    "                 transform: bool = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.processor = processor\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        self.split = split\n",
    "        self.do_crop = do_crop\n",
    "        if self.do_crop:\n",
    "            self.image_size = (int(image_size[0] * 2), int(image_size[1] * 4))\n",
    "            self.fov = fov\n",
    "            self.overlap_ratio = overlap_ratio\n",
    "            print(f\"Do Crop, Image size: {self.image_size}\")\n",
    "        else:\n",
    "            self.image_size = tuple(image_size)\n",
    "            print(f\"Do not Crop, Image size: {self.image_size}\")\n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, Union[torch.Tensor, str]]:\n",
    "        # 이미지 경로와 질문, 정답을 가져옵니다.\n",
    "        image_path = self.df.iloc[idx][\"url\"]\n",
    "        question = str(self.df.iloc[idx][\"query\"])\n",
    "        answer = str(self.df.iloc[idx][\"annotation\"])\n",
    "        \n",
    "        prompt = f\"Query: {question}\"\n",
    "        full_text = prompt + \" \" + \"Answer: \" + answer\n",
    "        # 이미지를 로드합니다.\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = self.processor(\n",
    "                images=image,\n",
    "                text=full_text,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "            )\n",
    "        # qtext = f\"Question: {question} Answer:\"\n",
    "        # 질문과 정답을 전처리합니다.\n",
    "        if self.do_crop:\n",
    "            inputs[\"pixel_values\"] = self.crop_equirectangular_tensor(inputs[\"pixel_values\"])\n",
    "        \n",
    "        # 정답을 전처리합니다.\n",
    "        labels = inputs.input_ids.clone()\n",
    "        # q_len = len(self.processor.tokenizer(prompt).input_ids)  # 질문+<image> token 개수\n",
    "        # labels[:, :q_len] = IGNORE_INDEX\n",
    "        labels[labels == self.processor.tokenizer.pad_token_id] = IGNORE_INDEX\n",
    "        \n",
    "        # 디버깅 (첫 번째 샘플에 대해서만)\n",
    "        if idx == 0:\n",
    "            print(\"==Input sequence==\")\n",
    "            print(inputs[\"input_ids\"][0])\n",
    "            print(self.processor.tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=False))\n",
    "            print(\"==Attention mask==\")\n",
    "            print(inputs[\"attention_mask\"][0])\n",
    "            print(\"==Labels==\")\n",
    "            print(labels[0])\n",
    "            \n",
    "        # Hugging Face Trainer가 기대하는 평평한 구조로 반환\n",
    "        return {\n",
    "            \"pixel_values\": inputs[\"pixel_values\"].squeeze(0),  # (Num Crops ,C, H, W)\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),        # (L1)\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),  # (L1)\n",
    "            \"labels\": labels.squeeze(0),          # (L2)\n",
    "            \"image_path\": image_path,\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        }\n",
    "\n",
    "    def crop_equirectangular_tensor(self, img_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        B, C, H2, W4 = img_tensor.shape\n",
    "        assert B == 1\n",
    "        H, W = H2 // 2, W4 // 4\n",
    "\n",
    "        # 1) stride 각도\n",
    "        step = self.fov * (1.0 - self.overlap_ratio)\n",
    "\n",
    "        # 2) 필요한 패치 개수\n",
    "        num_patches = int(np.ceil(360.0 / step))\n",
    "\n",
    "        # 3) 0도부터 시작해 step 간격으로 중심 각 생성\n",
    "        yaw_centers = (np.arange(num_patches) * step) % 360.0\n",
    "\n",
    "        # 4) e2p u_deg 인자용으로 -180~180 범위로 매핑\n",
    "        yaw_centers = np.where(yaw_centers > 180.0, yaw_centers - 360.0, yaw_centers)\n",
    "\n",
    "        # 5) numpy array 변환\n",
    "        img_np = img_tensor[0].permute(1, 2, 0).numpy()\n",
    "\n",
    "        patches = []\n",
    "        for u_deg in yaw_centers:\n",
    "            pers = e2p(\n",
    "                img_np,\n",
    "                fov_deg=self.fov,\n",
    "                u_deg=float(u_deg),\n",
    "                v_deg=0.0,\n",
    "                out_hw=(H, W),\n",
    "                in_rot_deg=0.0,\n",
    "                mode=\"bilinear\",\n",
    "            )  # (H, W, C)\n",
    "            t = torch.from_numpy(pers).permute(2, 0, 1)  # (C, H, W)\n",
    "            patches.append(t)\n",
    "\n",
    "        # (N, C, H, W) → (1, N, C, H, W)\n",
    "        return torch.stack(patches, dim=0).unsqueeze(0)\n",
    "    \n",
    "\n",
    "dataset = QuIC360Dataset(\n",
    "    csv_file=\"data/quic360/downtest.csv\",\n",
    "    processor=AutoProcessor.from_pretrained(\"google/siglip-base-patch16-224\"),\n",
    "    image_size=[224, 224],\n",
    "    max_length=128,\n",
    "    split=\"train\",\n",
    "    do_crop=False,\n",
    "    fov=90.0,\n",
    "    overlap_ratio=0.5,\n",
    "    transform=False\n",
    ")\n",
    "\n",
    "inputs = dataset[0]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def show_image_grid(images, ncols=4):\n",
    "    n_images = len(images)\n",
    "    n_rows = (n_images + ncols - 1) // ncols\n",
    "    fig, axes = plt.subplots(n_rows, ncols, figsize=(15, 5 * n_rows))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes]\n",
    "    \n",
    "    for i, img in enumerate(images):\n",
    "        axes[i].imshow(img.permute(1, 2, 0).numpy())\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# Show the cropped images\n",
    "show_image_grid(inputs[\"pixel_values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e39fe583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% that image 0 is 'This is a sidewalk near a large intersection in a commercial district of a city.  There are large trucks, a bus, and several cars waiting at a red traffic light.'\n"
     ]
    }
   ],
   "source": [
    "inputs = inputs.to(model.device)\n",
    "# run infernece\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)   \n",
    "    img_embeddings = outputs.image_embeds\n",
    "    text_embeddings = outputs.text_embeds \n",
    "\n",
    "logits_per_image = outputs.logits_per_image\n",
    "probs = torch.sigmoid(logits_per_image)\n",
    "print(f\"{probs[0][0]:.1%} that image 0 is '{answer}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sur",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
