{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f410f44a",
   "metadata": {},
   "source": [
    "#### 무변환 패치 자르기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5e79ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def extract_sliding_patches(\n",
    "    pil_img: Image.Image,\n",
    "    fov: float,\n",
    "    overlap_ratio: float\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pil_img (PIL.Image.Image): 입력 equirectangular 파노라마 이미지\n",
    "        fov (float): 한 패치의 시야각(FOV, degree 단위). ex) 90.0\n",
    "        overlap_ratio (float): 패치 간 겹침 비율 (0.0 ~ 1.0)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: (N, C, patch_h, patch_w) 형태의 패치 텐서 모음\n",
    "    \"\"\"\n",
    "    # 1) PIL → Torch 텐서 변환 (C, H2, W4), 값 범위 [0,1]\n",
    "    img_arr = np.array(pil_img)                              # (H2, W4, 3)\n",
    "    img_tensor = torch.from_numpy(img_arr).permute(2, 0, 1).float() / 255.0\n",
    "    # → (C=3, H2, W4)\n",
    "\n",
    "    C, H2, W4 = img_tensor.shape\n",
    "\n",
    "    # 2) 패치 높이(patch_h)와 패치 너비(patch_w) 계산\n",
    "    #    (예시에서는 patch_h를 H2/4로 가정 가능하나, 원본 코드는 외부에서 지정됨)\n",
    "    #    여기서는 단순히 “중앙 영역 높이”를 patch_h로 사용한다고 가정\n",
    "    patch_h = int(np.floor(H2 / 2.0))  # 예: 중앙 50% 높이만 사용하기 위한 임시 값\n",
    "    patch_w = int(W4 * (fov / 360.0))  # FOV 기반으로 패치 가로 픽셀 수 결정\n",
    "\n",
    "    # 3) 스트라이드 계산 (겹침 비율 반영)\n",
    "    stride_h = max(int(patch_h * (1.0 - overlap_ratio)), 1)\n",
    "    stride_w = max(int(patch_w * (1.0 - overlap_ratio)), 1)\n",
    "\n",
    "    # 4) 상단 H2//4, 하단 H2//4 영역 제외할 세로 범위 계산\n",
    "    y_min = int(np.ceil(H2 / 4.0))\n",
    "    y_max = int(H2 - np.floor(H2 / 4.0) - patch_h)\n",
    "\n",
    "    patches = []\n",
    "    if y_max >= y_min:\n",
    "        for y in range(y_min, y_max + 1, stride_h):\n",
    "            for x in range(0, W4, stride_w):\n",
    "                if x + patch_w <= W4:\n",
    "                    # 우측 범위 내에 완전히 포함되는 경우\n",
    "                    patch = img_tensor[:, y : y + patch_h, x : x + patch_w]\n",
    "                else:\n",
    "                    # 우측을 넘어가는 경우: 우측 일부 + 좌측 일부 이어붙이기 (래핑)\n",
    "                    right_part = img_tensor[:, y : y + patch_h, x : W4]\n",
    "                    left_part  = img_tensor[:, y : y + patch_h, 0 : (x + patch_w) - W4]\n",
    "                    patch = torch.cat([right_part, left_part], dim=2)\n",
    "\n",
    "                patches.append(patch)\n",
    "\n",
    "    # 5) (N, C, patch_h, patch_w) 형태로 합치기\n",
    "    if len(patches) == 0:\n",
    "        # 유효 패치가 없는 경우, 빈 텐서 반환\n",
    "        return torch.zeros((0, C, patch_h, patch_w))\n",
    "    else:\n",
    "        return torch.stack(patches, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c348f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "from src.utils import visualize_tensor_batch\n",
    "from train_vlm import QuIC360Dataset\n",
    "from transformers import AutoProcessor, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/dinov2-small\")\n",
    "print(processor)\n",
    "tokenzier = AutoTokenizer.from_pretrained(\"google/gemma-3-4b-it\")\n",
    "img = Image.open(\"data/quic360/downtest/images/540231919_58d07745aa_o.jpg\").convert(\"RGB\")\n",
    "img = img.resize((224*4, 224*2), Image.BILINEAR)\n",
    "img_tensor = torch.from_numpy(np.array(img)).permute(2, 0, 1).float() / 255.0\n",
    "img_tensor = img_tensor.unsqueeze(0)  # (1, C, H2, W4)\n",
    "dataset = QuIC360Dataset(\n",
    "    csv_file=\"data/quic360/downtest.csv\",\n",
    "    image_processor= processor,\n",
    "    tokenizer=tokenzier,\n",
    "    image_size=[224, 224],\n",
    "    max_length=128,\n",
    "    do_crop=True,\n",
    "    fov=90,\n",
    "    overlap_ratio=0.5,\n",
    ")\n",
    "imgs = dataset.crop_equirectangular_tensor(img_tensor)\n",
    "visualize_tensor_batch(imgs)\n",
    "sample = processor(images=imgs, return_tensors=\"pt\")\n",
    "model = AutoModel.from_pretrained('facebook/dinov2-small',cache_dir='./.cache')\n",
    "outputs = model(sample['pixel_values'])\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "last_hidden_states = last_hidden_states.cpu().detach().numpy()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 무변환 크롭 특징 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47298436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "# from train import QuIC360Dataset\n",
    "from transformers import AutoProcessor, AutoTokenizer\n",
    "from src.utils import visualize_tensor_batch\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/dinov2-small\")\n",
    "model = AutoModel.from_pretrained('facebook/dinov2-small',cache_dir='./.cache')\n",
    "img = Image.open(\"data/quic360/downtest/images/540231919_58d07745aa_o.jpg\").convert(\"RGB\")\n",
    "imgs = extract_sliding_patches(\n",
    "    pil_img=img,\n",
    "    fov=90.0,\n",
    "    overlap_ratio=0.5\n",
    ")\n",
    "visualize_tensor_batch(imgs)\n",
    "sample = processor(\n",
    "    images=imgs,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "outputs = model(sample['pixel_values'])\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "last_hidden_states = last_hidden_states.cpu().detach().numpy()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e2cea3",
   "metadata": {},
   "source": [
    "### 큐브맵 변환 특징추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724b3b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def extract_edge_patches_from_panorama(\n",
    "    pil_img: Image.Image,\n",
    "    patch_size: int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    equirectangular 파노라마 이미지를 변환하여, 상·하단 면을 제외한 4개 면(front, right, back, left)에서\n",
    "    서로 맞닿는 4개 모서리마다 각 면에서 하나씩, 총 8개의 정사각 패치를 추출하여 반환합니다.\n",
    "\n",
    "    Args:\n",
    "        pil_img (PIL.Image.Image):\n",
    "            입력 equirectangular 파노라마 이미지\n",
    "        patch_size (int):\n",
    "            추출할 정사각 패치의 한 변 길이(픽셀).\n",
    "            patch_size <= face_size 이어야 합니다. (face_size = H//2)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor:\n",
    "            (8, C, patch_size, patch_size) 형태의 패치 묶음. 순서는 아래와 같습니다.\n",
    "                1. front 우측 (edge with right)\n",
    "                2. right 앞쪽 (edge with front)\n",
    "                3. right 우측 (edge with back)\n",
    "                4. back  앞쪽 (edge with right)\n",
    "                5. back  우측 (edge with left)\n",
    "                6. left  앞쪽 (edge with back)\n",
    "                7. left  우측 (edge with front)\n",
    "                8. front 앞쪽 (edge with left)\n",
    "    \"\"\"\n",
    "    # --- 1) PIL → Torch Tensor (C, H, W), [0,1] 정규화 ---\n",
    "    img_arr    = np.array(pil_img)                              # (H, W, 3)\n",
    "    img_tensor = torch.from_numpy(img_arr).permute(2, 0, 1).float() / 255.0\n",
    "    C, H, W    = img_tensor.shape\n",
    "\n",
    "    # --- 2) face_size 계산 (정사각 큐브 면 크기) ---\n",
    "    face_size = H // 2\n",
    "    assert patch_size <= face_size, \"patch_size는 face 크기(face_size)보다 작거나 같아야 합니다.\"\n",
    "\n",
    "    # --- 3) equirectangular → (1, C, H, W) ---\n",
    "    eq_tensor = img_tensor.unsqueeze(0)\n",
    "\n",
    "    # --- 4) u, v 그리드 생성 (각 면 크기: face_size × face_size) ---\n",
    "    device = eq_tensor.device\n",
    "    lin = torch.linspace(-1.0, 1.0, face_size, device=device)\n",
    "    u_grid, v_grid = torch.meshgrid(lin, -lin, indexing=\"xy\")\n",
    "\n",
    "    # --- 5) 4개 면(front, right, back, left)의 방향벡터 계산 + 정규화 ---\n",
    "    def _dir_face(name):\n",
    "        if name == \"+Z\":   # front\n",
    "            x = u_grid;     y = v_grid;     z = torch.ones_like(u_grid)\n",
    "        elif name == \"+X\": # right\n",
    "            x = torch.ones_like(u_grid);     y = v_grid;     z = -u_grid\n",
    "        elif name == \"-Z\": # back\n",
    "            x = -u_grid;    y = v_grid;     z = -torch.ones_like(u_grid)\n",
    "        elif name == \"-X\": # left\n",
    "            x = -torch.ones_like(u_grid);    y = v_grid;     z = u_grid\n",
    "        else:\n",
    "            raise ValueError(\"지원되지 않는 면 이름\")\n",
    "        vec = torch.stack([x, y, z], dim=-1)      # (face_size, face_size, 3)\n",
    "        norm = torch.linalg.norm(vec, dim=-1, keepdim=True)\n",
    "        return vec / norm                         # (face_size, face_size, 3)\n",
    "\n",
    "    face_order = [\"+Z\", \"+X\", \"-Z\", \"-X\"]  # [front, right, back, left]\n",
    "    dirs = torch.stack([_dir_face(f) for f in face_order], dim=0)  \n",
    "    # → (4, face_size, face_size, 3)\n",
    "\n",
    "    # --- 6) 구면 좌표(lon, lat)로 변환 ---\n",
    "    x, y, z = dirs[..., 0], dirs[..., 1], dirs[..., 2]\n",
    "    lon = torch.atan2(z, x)    # (4, face_size, face_size)\n",
    "    lat = torch.asin(y)        # (4, face_size, face_size)\n",
    "\n",
    "    # --- 7) equirectangular UV 매핑 (u_eq, v_eq) ---\n",
    "    u_eq = (lon / (2.0 * np.pi) + 0.5) * (W - 1)\n",
    "    v_eq = (0.5 - lat / np.pi) * (H - 1)\n",
    "\n",
    "    # --- 8) grid_sample 입력용 [-1, +1] 정규화 그리드 생성 ---\n",
    "    x_norm = (2.0 * u_eq / (W - 1)) - 1.0\n",
    "    y_norm = (2.0 * v_eq / (H - 1)) - 1.0\n",
    "    grid = torch.stack([x_norm, y_norm], dim=-1)  # (4, face_size, face_size, 2)\n",
    "\n",
    "    # --- 9) grid_sample → 4개 면 텐서 추출 ---\n",
    "    eq_repeat = eq_tensor.repeat(4, 1, 1, 1)  # (4, C, H, W)\n",
    "    cube4 = F.grid_sample(\n",
    "        eq_repeat,    # (4, C, H, W)\n",
    "        grid,         # (4, face_size, face_size, 2)\n",
    "        mode=\"bilinear\",\n",
    "        padding_mode=\"border\",\n",
    "        align_corners=True\n",
    "    )\n",
    "    # cube4: (4, C, face_size, face_size)\n",
    "\n",
    "    # --- 10) 네 면에서 모서리 패치 추출 ---\n",
    "    # 각 면 인덱스\n",
    "    idx_front = 0  # +Z\n",
    "    idx_right = 1  # +X\n",
    "    idx_back  = 2  # -Z\n",
    "    idx_left  = 3  # -X\n",
    "\n",
    "    # 세로 중앙 영역 계산\n",
    "    y_center = face_size // 2\n",
    "    y_start  = y_center - (patch_size // 2)\n",
    "    y_end    = y_start + patch_size  # exclusive\n",
    "\n",
    "    front = cube4[idx_front]  # (C, F, F)\n",
    "    right = cube4[idx_right]\n",
    "    back  = cube4[idx_back]\n",
    "    left  = cube4[idx_left]\n",
    "\n",
    "    # 1) front 우측  <-> right 앞쪽\n",
    "    front_patch_right = front[:, y_start : y_end, (face_size - patch_size) : face_size]\n",
    "    right_patch_front = right[:, y_start : y_end, 0 : patch_size]\n",
    "\n",
    "    # 2) right 우측  <-> back 앞쪽\n",
    "    right_patch_right = right[:, y_start : y_end, (face_size - patch_size) : face_size]\n",
    "    back_patch_front   = back[:, y_start : y_end, 0 : patch_size]\n",
    "\n",
    "    # 3) back 우측  <-> left 앞쪽\n",
    "    back_patch_right  = back[:, y_start : y_end, (face_size - patch_size) : face_size]\n",
    "    left_patch_front  = left[:, y_start : y_end, 0 : patch_size]\n",
    "\n",
    "    # 4) left 우측  <-> front 앞쪽\n",
    "    left_patch_right  = left[:, y_start : y_end, (face_size - patch_size) : face_size]\n",
    "    front_patch_front = front[:, y_start : y_end, 0 : patch_size]\n",
    "\n",
    "    patches = [\n",
    "        front_patch_right,  # 1)\n",
    "        right_patch_front,  # 2)\n",
    "        right_patch_right,  # 3)\n",
    "        back_patch_front,   # 4)\n",
    "        back_patch_right,   # 5)\n",
    "        left_patch_front,   # 6)\n",
    "        left_patch_right,   # 7)\n",
    "        front_patch_front   # 8)\n",
    "    ]\n",
    "\n",
    "    return torch.stack(patches, dim=0)  # (8, C, patch_size, patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cc1d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "# from train import QuIC360Dataset\n",
    "from transformers import AutoProcessor, AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/dinov2-small\")\n",
    "model = AutoModel.from_pretrained('facebook/dinov2-small',cache_dir='./.cache')\n",
    "img = Image.open(\"data/quic360/downtest/images/540231919_58d07745aa_o.jpg\").convert(\"RGB\")\n",
    "imgs = extract_sliding_patches(\n",
    "    pil_img=img,\n",
    "    fov=90.0,\n",
    "    overlap_ratio=0.5\n",
    ")\n",
    "sample = processor(\n",
    "    images=imgs,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "outputs = model(sample['pixel_values'])\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "last_hidden_states = last_hidden_states.cpu().detach().numpy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cba62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "from src.utils import visualize_tensor_batch\n",
    "# from train import QuIC360Dataset\n",
    "from transformers import AutoProcessor, AutoTokenizer\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/dinov2-small\")\n",
    "model = AutoModel.from_pretrained('facebook/dinov2-small',cache_dir='./.cache')\n",
    "img = Image.open(\"data/quic360/downtest/images/540231919_58d07745aa_o.jpg\").convert(\"RGB\")\n",
    "\n",
    "imgs = extract_edge_patches_from_panorama(\n",
    "    pil_img=img,\n",
    "    patch_size=224\n",
    ")\n",
    "visualize_tensor_batch(imgs)\n",
    "sample = processor(\n",
    "    images=imgs,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "outputs = model(sample['pixel_values'])\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "last_hidden_states = last_hidden_states.cpu().detach().numpy()  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
