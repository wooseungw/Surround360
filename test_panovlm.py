import os
import torch
import numpy as np
from PIL import Image
from transformers import AutoTokenizer
from src.models.panovlm import PanoVLM
from src.models.panovlm_config import PanoVLMConfig
from torchinfo import summary

def test_panovlm_initialization():
    """PanoVLM ëª¨ë¸ì˜ ì´ˆê¸°í™”ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    config = PanoVLMConfig(
        vision_model_name_or_path="facebook/dinov2-small",
        language_model_name_or_path="google/gemma-3-4b-it",
        use_fp16=True
    )
    
    print("ğŸ”„ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    model = PanoVLM(config)
    print("ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ!")
    
    print("\nğŸ“‹ Model summary:")
    print("===" * 20)
    print(f"ğŸ§  Language Model: {type(model.language_model).__name__}")
    print(f"ğŸ‘ï¸ Vision Model: {type(model.vision_model).__name__}")
    print(f"ğŸ”„ Projector: {model.projector}")
    print("===" * 20)
    
    assert model.vision_model is not None, "Vision model should be initialized."
    assert model.language_model is not None, "Language model should be initialized."
    assert hasattr(model, 'projector'), "Projector should be defined."
    
    # Check if the language model is correctly extracted
    text_model = model.language_model
    assert text_model is not None, "Text model should be extracted from the full VLM."
    
    print("âœ… PanoVLM initialization test passed.")
    return model, config

def test_panovlm_forward(model, batch_size=1, num_patches=4, use_cuda=False):
    """ëª¨ë¸ì˜ forward passë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    print("\nğŸ”„ Testing forward pass...")
    
    # ë‘ ê°€ì§€ í˜•íƒœì˜ ì…ë ¥ í…ŒìŠ¤íŠ¸
    print("1. íŒŒë…¸ë¼ë§ˆ ì´ë¯¸ì§€ ì…ë ¥ í…ŒìŠ¤íŠ¸ ([B, P, C, H, W])")
    
    # ì˜ˆì‹œ ì…ë ¥ í…ì„œ ìƒì„± (íŒŒë…¸ë¼ë§ˆ ì´ë¯¸ì§€ ì‹œë®¬ë ˆì´ì…˜)
    # [B, P, C, H, W] í˜•íƒœì˜ í…ì„œ ìƒì„± (ë°°ì¹˜, íŒ¨ì¹˜, ì±„ë„, ë†’ì´, ë„ˆë¹„)
    batch_size = batch_size  # ë°°ì¹˜ í¬ê¸°
    num_patches = num_patches  # íŒŒë…¸ë¼ë§ˆ íŒ¨ì¹˜ ìˆ˜
    channels = 3  # RGB
    height = 224  # ë†’ì´
    width = 224  # ë„ˆë¹„
    
    # ëœë¤ í”½ì…€ ê°’ ìƒì„± ë° ì •ê·œí™” (ë” í˜„ì‹¤ì ì¸ ê°’)
    pixel_values = torch.rand(batch_size, num_patches, channels, height, width)
    # í”½ì…€ê°’ ë²”ìœ„ë¥¼ [-1, 1]ë¡œ ì¡°ì • (ì¼ë°˜ì ì¸ ì •ê·œí™”)
    pixel_values = (pixel_values * 2) - 1
    
    # ì˜ˆì‹œ í† í¬ë‚˜ì´ì € (ì‹¤ì œ ëª¨ë¸ì— ë§ëŠ” í† í¬ë‚˜ì´ì € ì‚¬ìš© í•„ìš”)
    try:
        tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-4b-it", trust_remote_code=True)
        # í…ìŠ¤íŠ¸ ì¸ì½”ë”©
        text = "<image> ì´ íŒŒë…¸ë¼ë§ˆ ì´ë¯¸ì§€ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."
        input_ids = tokenizer(text, return_tensors="pt").input_ids
        attention_mask = torch.ones_like(input_ids)
    except Exception as e:
        print(f"âš ï¸ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}. ì„ì‹œ í…ì„œë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        input_ids = torch.randint(0, 1000, (batch_size, 10))
        attention_mask = torch.ones_like(input_ids)
    
    # CUDA ì‚¬ìš© ì—¬ë¶€ í™•ì¸
    if use_cuda and torch.cuda.is_available():
        model = model.to("cuda")
        pixel_values = pixel_values.to("cuda")
        input_ids = input_ids.to("cuda")
        attention_mask = attention_mask.to("cuda")
        print("ğŸ”¥ Using CUDA for inference")
    
    # ì…ì¶œë ¥ í˜•íƒœ í™•ì¸ìš© ì¶œë ¥
    print(f"ğŸ“Š Input shapes:")
    print(f"  - Pixel values: {pixel_values.shape}")
    print(f"  - Input IDs: {input_ids.shape}")
    
    # Forward pass ì‹¤í–‰
    try:
        with torch.no_grad():
            outputs = model(
                pixel_values=pixel_values,
                input_ids=input_ids,
                attention_mask=attention_mask,
            )
        
        print(f"ğŸ“Š Output shape: {outputs.shape if hasattr(outputs, 'shape') else 'Not a tensor'}")
        print("âœ… Forward pass succeeded!")
        return True
    except Exception as e:
        print(f"âŒ Forward pass failed: {e}")
        return False

def test_panovlm_generate(model, batch_size=1, num_patches=4, use_cuda=False):
    """ëª¨ë¸ì˜ generate í•¨ìˆ˜ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    print("\nğŸ”„ Testing generate function...")
    
    # íŒŒë…¸ë¼ë§ˆ ì´ë¯¸ì§€ ì‹œë®¬ë ˆì´ì…˜
    pixel_values = torch.rand(batch_size, num_patches, 3, 224, 224)
    # í”½ì…€ê°’ ë²”ìœ„ë¥¼ [-1, 1]ë¡œ ì¡°ì • (ì¼ë°˜ì ì¸ ì •ê·œí™”)
    pixel_values = (pixel_values * 2) - 1
    
    # í”„ë¡¬í”„íŠ¸ ì„¤ì •
    try:
        tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-4b-it", trust_remote_code=True)
        prompt = "ì´ íŒŒë…¸ë¼ë§ˆ ì´ë¯¸ì§€ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”:"
        input_ids = tokenizer(prompt, return_tensors="pt").input_ids
        attention_mask = torch.ones_like(input_ids)
    except Exception as e:
        print(f"âš ï¸ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}. ì„ì‹œ í…ì„œë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        input_ids = torch.randint(0, 1000, (batch_size, 10))
        attention_mask = torch.ones_like(input_ids)
    
    # CUDA ì‚¬ìš© ì—¬ë¶€ í™•ì¸
    if use_cuda and torch.cuda.is_available():
        model = model.to("cuda")
        pixel_values = pixel_values.to("cuda")
        input_ids = input_ids.to("cuda")
        attention_mask = attention_mask.to("cuda")
    
    # ìƒì„± ì„¤ì •
    gen_kwargs = {
        "max_new_tokens": 20,
        "do_sample": True,
        "temperature": 0.7,
        "top_p": 0.9,
    }
    
    # Generate ì‹¤í–‰
    try:
        with torch.no_grad():
            generated_ids = model.generate(
                pixel_values=pixel_values,
                input_ids=input_ids,
                attention_mask=attention_mask,
                **gen_kwargs
            )
        
        print(f"ğŸ“Š Generated IDs shape: {generated_ids.shape}")
        
        # í† í¬ë‚˜ì´ì €ë¡œ ë””ì½”ë”© (ê°€ëŠ¥í•œ ê²½ìš°)
        try:
            generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
            print(f"ğŸ“ Generated text sample: {generated_text[0][:50]}...")
        except Exception as e:
            print(f"âš ï¸ ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”© ì‹¤íŒ¨: {e}")
        
        print("âœ… Generate function succeeded!")
        return True
    except Exception as e:
        print(f"âŒ Generate function failed: {e}")
        return False

def test_with_real_image(model, image_path=None, use_cuda=False):
    """ì‹¤ì œ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    # ì´ë¯¸ì§€ê°€ ì œê³µë˜ì§€ ì•Šì€ ê²½ìš° ìƒ˜í”Œ ì´ë¯¸ì§€ ì°¾ê¸°
    if image_path is None:
        sample_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "samples")
        if os.path.exists(sample_dir):
            images = [f for f in os.listdir(sample_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]
            if images:
                image_path = os.path.join(sample_dir, images[0])
    
    if not image_path or not os.path.exists(image_path):
        print("âŒ í…ŒìŠ¤íŠ¸í•  ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹¤ì œ ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return False
    
    print(f"\nğŸ–¼ï¸ Testing with real image: {image_path}")
    
    try:
        # ì´ë¯¸ì§€ ë¡œë“œ
        from PIL import Image
        import torchvision.transforms as T
        
        image = Image.open(image_path).convert("RGB")
        
        # ì „ì²˜ë¦¬
        transform = T.Compose([
            T.Resize((224, 224)),
            T.ToTensor(),
            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        # íŒŒë…¸ë¼ë§ˆ ì´ë¯¸ì§€ ì‹œë®¬ë ˆì´ì…˜ - [B, P, C, H, W] í˜•íƒœ
        # ë°©ë²• 1: ë‹¨ì¼ ì´ë¯¸ì§€ë¥¼ ì—¬ëŸ¬ íŒ¨ì¹˜ë¡œ ë³µì œ (ìƒ˜í”Œë§ëœ íŒŒë…¸ë¼ë§ˆ ë·° ì‹œë®¬ë ˆì´ì…˜)
        img_tensor = transform(image)
        patches = []
        for i in range(4):  # 4ê°œì˜ íŒ¨ì¹˜/ë·° ì‹œë®¬ë ˆì´ì…˜
            patches.append(img_tensor)
        patches = torch.stack(patches, dim=0).unsqueeze(0)  # [1, 4, 3, 224, 224]
        
        # í† í¬ë‚˜ì´ì € ë° í”„ë¡¬í”„íŠ¸ ì„¤ì •
        try:
            tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-4b-it", trust_remote_code=True)
            prompt = "ì´ íŒŒë…¸ë¼ë§ˆ ì´ë¯¸ì§€ì— ë¬´ì—‡ì´ ë³´ì´ë‚˜ìš”?"
            input_ids = tokenizer(prompt, return_tensors="pt").input_ids
            attention_mask = torch.ones_like(input_ids)
        except Exception as e:
            print(f"âš ï¸ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}. ì„ì‹œ í…ì„œë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            input_ids = torch.randint(0, 1000, (1, 10))
            attention_mask = torch.ones_like(input_ids)
        
        # CUDA ì‚¬ìš© ì—¬ë¶€ í™•ì¸
        if use_cuda and torch.cuda.is_available():
            model = model.to("cuda")
            patches = patches.to("cuda")
            input_ids = input_ids.to("cuda")
            attention_mask = attention_mask.to("cuda")
        
        # ìƒì„±
        with torch.no_grad():
            generated_ids = model.generate(
                pixel_values=patches,
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=30,
            
            )
        
        # ê²°ê³¼ ë””ì½”ë”©
        try:
            generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
            print(f"ğŸ“ Generated text: {generated_text[0]}")
        except Exception as e:
            print(f"âš ï¸ ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”© ì‹¤íŒ¨: {e}")
        
        print("âœ… Real image test succeeded!")
        return True
    except Exception as e:
        print(f"âŒ Real image test failed: {e}")
        return False

if __name__ == "__main__":
    print("ğŸš€ PanoVLM í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
    model, config = test_panovlm_initialization()
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ CUDA í™•ì¸
    use_cuda = torch.cuda.is_available()
    if use_cuda:
        print(f"âœ… CUDA available: {torch.cuda.get_device_name(0)}")
    else:
        print("âš ï¸ CUDA not available, using CPU")
    
    # Forward pass í…ŒìŠ¤íŠ¸
    success_forward = test_panovlm_forward(model, use_cuda=use_cuda)
    
    # Generate í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
    if success_forward:  # Forwardê°€ ì„±ê³µí•œ ê²½ìš°ë§Œ Generate í…ŒìŠ¤íŠ¸
        success_generate = test_panovlm_generate(model, use_cuda=use_cuda)
    
    # ì‹¤ì œ ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸ (ì„ íƒì )
    # ìƒ˜í”Œ ì´ë¯¸ì§€ í´ë”ê°€ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
    test_with_real_image(model, use_cuda=use_cuda)
    
    print("\nğŸ PanoVLM í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
